{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.查看服务器GPU等硬件信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.缓存云盘上的模型和数据集到服务器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "# 数据集的名称\n",
    "data_set='CCF_2019.zip'\n",
    "# 模型的路径\n",
    "model='/content/gdrive/MyDrive/Models/pytorchmodels/FinBERT_L-12_H-768_A-12'\n",
    "\n",
    "# 解压缩\n",
    "def unzip(file_path,tar_path):\n",
    "\n",
    "    # 压缩文件判断\n",
    "    if os.path.splitext(file_path)[-1]=='.zip':\n",
    "\n",
    "        zFile = zipfile.ZipFile(file_path, \"r\")\n",
    "        for files in zFile.namelist(): \n",
    "            zFile.extract(files,tar_path)\n",
    "        \n",
    "        zFile.close()\n",
    "\n",
    "# 下载到服务器\n",
    "def down_data(dataset_name,model_path):\n",
    "\n",
    "    GoogleDrive_PATH='/content/gdrive/MyDrive/DataSets/'\n",
    "    Local_PATH='/content/LocalDataSets/'\n",
    "    Local_Model_PATH='/content/LocalModels/'\n",
    "\n",
    "    DataSetList=os.listdir(GoogleDrive_PATH)   \n",
    "    print('DataSetList:',DataSetList)\n",
    "\n",
    "    # 下载数据集\n",
    "    if dataset_name in DataSetList:\n",
    "\n",
    "        if not os.path.exists(Local_PATH):\n",
    "            os.makedirs(Local_PATH)\n",
    "            \n",
    "        shutil.copy(GoogleDrive_PATH+dataset_name,Local_PATH+dataset_name)\n",
    "\n",
    "        # 如果是压缩文件\n",
    "        unzip(Local_PATH+dataset_name,Local_PATH)\n",
    "    \n",
    "    # 下载模型\n",
    "    try:\n",
    "        shutil.copytree(model_path,Local_Model_PATH+str(model_path).split('/')[-1])\n",
    "    except FileExistsError as e:\n",
    "        pass\n",
    "    \n",
    "down_data(data_set,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.在服务器上安装环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers datasets tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.在预训练模型上进行微调"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  可视化\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "log_writer = SummaryWriter()\n",
    "\n",
    "run_path=''\n",
    "%tensorboard --logdir=/Users/mac/PycharmProjects/pythonProject/saved/runs/{run_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#-------------------- 模型和数据集路径 --------------------#\n",
    "model_path='/content/LocalModels/FinBERT_L-12_H-768_A-12/'\n",
    "dataset_path='/content/LocalDataSets/CCF_2019/'\n",
    "saved_path='/content/gdrive/MyDrive/Saved_Models/FinBERT_L-12_H-768_A-12/'\n",
    "if not os.path.exists(saved_path):\n",
    "        os.makedirs(saved_path)\n",
    "\n",
    "# 读取数据\n",
    "def read_data(base_url):\n",
    "    return load_dataset('csv',\n",
    "                        data_files={'train': base_url + 'train.csv',\n",
    "                                    'test': base_url + 'test.csv',\n",
    "                                    'dev': base_url + 'dev.csv'})\n",
    "\n",
    "\n",
    "# 编码训练集\n",
    "def tokenize_data(tokenizer:BertTokenizer,word_length:int=32):\n",
    "    # 加载数据集\n",
    "    raw_datasets = read_data(dataset_path)\n",
    "\n",
    "    # 向量化函数\n",
    "    def tokenize_function(dataset):\n",
    "        return tokenizer(dataset['title'], truncation=True, padding='max_length', max_length=word_length)\n",
    "\n",
    "    tokenized_datasets = raw_datasets.map(tokenize_function,\n",
    "                                          batched=True)\n",
    "\n",
    "    # 重命名列\n",
    "    tokenized_datasets = tokenized_datasets.rename_column('label', 'labels')\n",
    "\n",
    "    return tokenized_datasets\n",
    "\n",
    "\n",
    "#-------------------- 模型训练 --------------------#\n",
    "def train_model(bert_path,class_num:int=3):\n",
    "\n",
    "    # 获取预训练的编码器和模型\n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_path)\n",
    "    model = BertForSequenceClassification.from_pretrained(bert_path, num_labels=class_num)\n",
    "\n",
    "    # 获得向量化后的数据\n",
    "    tokenized_datasets = tokenize_data(tokenizer)\n",
    "\n",
    "    # 定义评价指标\n",
    "    def compute_metrics(eval_preds):\n",
    "        logits, labels = eval_preds\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='micro')\n",
    "        acc = accuracy_score(labels, predictions)\n",
    "        result = {\n",
    "            'accuracy': acc,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    #-------------------- 定义训练参数 --------------------#\n",
    "\n",
    "    # output_dir = './saved/FinBERT'\n",
    "    # tensorboard --logdir ./saved/FinBERT/runs\n",
    "    args = TrainingArguments(\n",
    "        output_dir=saved_path,  # 保存路径，存放检查点和其他输出文件\n",
    "        evaluation_strategy='steps',  # 每50steps结束后进行评价\n",
    "        eval_steps=50,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        report_to=\"tensorboard\",\n",
    "        # warmup_steps=500,  # 热身步数\n",
    "        # weight_decay=0.01,  # 权重衰减\n",
    "        learning_rate=2e-5,  # 初始学习率\n",
    "        per_device_train_batch_size=64,  # 训练批次大小\n",
    "        per_device_eval_batch_size=64,  # 测试批次大小\n",
    "        num_train_epochs=4,  # 训练轮数\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "    # 定义训练器\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=tokenized_datasets['train'],\n",
    "        eval_dataset=tokenized_datasets[\"dev\"],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # log_writer = SummaryWriter()\n",
    "\n",
    "    # tensorboard --logdir=/Users/mac/PycharmProjects/pythonProject/saved/runs/Jul09_23-01-26_localhost\n",
    "    # tensorboard dev upload --logdir '/Users/mac/PycharmProjects/pythonProject/saved/runs/Jul09_23-01-26_localhost'\n",
    "\n",
    "    # 开始训练\n",
    "    trainer.train()\n",
    "\n",
    "    # 训练完成以后的测试集评价\n",
    "    trainer.evaluate(eval_dataset=tokenized_datasets['test'])\n",
    "\n",
    "train_model(model_path,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推理任务"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
