{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.服务器环境配置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 安装需要的软件包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers datasets tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 配置Git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From https://github.com/Colin-Bao/Colab-Notebooks\n",
      " * branch            master     -> FETCH_HEAD\n",
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "!git pull --tags origin master\n",
    "!git config --global user.email \"Colin-Bao@github.com\"\n",
    "!git config --global user.name \"Colin-Bao\"\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看服务器 GPU 等硬件信息\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.缓存云盘上的模型和数据集到服务器\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "# 云盘的路径\n",
    "Cloud_Data_Path = '/content/gdrive/MyDrive/DataSets/'\n",
    "Data_Set_Name = 'CCF_2019.zip'  # 数据集\n",
    "Cloud_Model_Path = '/content/gdrive/MyDrive/Models/pytorchmodels/FinBERT_L-12_H-768_A-12/'  # 模型\n",
    "\n",
    "# 服务器的路径\n",
    "Local_Data_Path = '/content/LocalDataSets/'\n",
    "Local_Model_Path = '/content/LocalModels/'\n",
    "\n",
    "# 解压缩\n",
    "\n",
    "\n",
    "def unzip(file_path, tar_path):\n",
    "\n",
    "    # 压缩文件判断\n",
    "    if os.path.splitext(file_path)[-1] == '.zip':\n",
    "\n",
    "        zFile = zipfile.ZipFile(file_path, \"r\")\n",
    "        for files in zFile.namelist():\n",
    "            zFile.extract(files, tar_path)\n",
    "\n",
    "        zFile.close()\n",
    "\n",
    "# 下载到服务器\n",
    "\n",
    "\n",
    "def down_data():\n",
    "\n",
    "    # 列出云盘所有数据集\n",
    "    DataSetList = os.listdir(Cloud_Data_Path)\n",
    "    print('DataSetList:', DataSetList)\n",
    "\n",
    "    # 下载数据集\n",
    "    if Data_Set_Name in DataSetList:\n",
    "\n",
    "        # 创建本地数据集文件夹\n",
    "        if not os.path.exists(Local_Data_Path):\n",
    "            os.makedirs(Local_Data_Path)\n",
    "\n",
    "        # 复制到本地\n",
    "        shutil.copy(Cloud_Data_Path+Data_Set_Name,\n",
    "                    Local_Data_Path+Data_Set_Name)\n",
    "\n",
    "        # 如果是压缩文件\n",
    "        unzip(Local_Data_Path+Data_Set_Name, Local_Data_Path)\n",
    "\n",
    "    # 下载模型\n",
    "    try:\n",
    "        shutil.copytree(Cloud_Model_Path, Local_Model_Path +\n",
    "                        str(Cloud_Model_Path).split('/')[-2])\n",
    "    except FileExistsError as e:\n",
    "        pass\n",
    "\n",
    "\n",
    "down_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.在预训练模型上进行微调\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可视化训练过程\n",
    "用tensorboard记录日志文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  可视化\n",
    "logs_base_dir = '/content/gdrive/MyDrive/Saved_Models/FinBERT_L-12_H-768_A-12/runs/Jul22_13-49-58_1f15589edb2a/'\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir={logs_base_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 微调\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#-------------------- 模型和数据集路径 --------------------#\n",
    "model_path = '/content/LocalModels/FinBERT_L-12_H-768_A-12/'\n",
    "dataset_path = '/content/LocalDataSets/CCF_2019/'\n",
    "saved_path = '/content/gdrive/MyDrive/Saved_Models/FinBERT_L-12_H-768_A-12/'\n",
    "if not os.path.exists(saved_path):\n",
    "    os.makedirs(saved_path)\n",
    "\n",
    "# 读取数据\n",
    "\n",
    "\n",
    "def read_data(base_url):\n",
    "    return load_dataset('csv',\n",
    "                        data_files={'train': base_url + 'train.csv',\n",
    "                                    'test': base_url + 'test.csv',\n",
    "                                    'dev': base_url + 'dev.csv'})\n",
    "\n",
    "\n",
    "# 编码训练集\n",
    "def tokenize_data(tokenizer: BertTokenizer, word_length: int = 32):\n",
    "    # 加载数据集\n",
    "    raw_datasets = read_data(dataset_path)\n",
    "\n",
    "    # 向量化函数\n",
    "    def tokenize_function(dataset):\n",
    "        return tokenizer(dataset['title'], truncation=True, padding='max_length', max_length=word_length)\n",
    "\n",
    "    tokenized_datasets = raw_datasets.map(tokenize_function,\n",
    "                                          batched=True)\n",
    "\n",
    "    # 重命名列\n",
    "    tokenized_datasets = tokenized_datasets.rename_column('label', 'labels')\n",
    "\n",
    "    return tokenized_datasets\n",
    "\n",
    "\n",
    "#-------------------- 模型训练 --------------------#\n",
    "def train_model(bert_path, class_num: int = 3):\n",
    "\n",
    "    # 获取预训练的编码器和模型\n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_path)\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        bert_path, num_labels=class_num)\n",
    "\n",
    "    # 获得向量化后的数据\n",
    "    tokenized_datasets = tokenize_data(tokenizer)\n",
    "\n",
    "    # 定义评价指标\n",
    "    def compute_metrics(eval_preds):\n",
    "        logits, labels = eval_preds\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            labels, predictions, average='micro')\n",
    "        acc = accuracy_score(labels, predictions)\n",
    "        result = {\n",
    "            'accuracy': acc,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "\n",
    "        return result\n",
    "\n",
    "    #-------------------- 定义训练参数 --------------------#\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=saved_path,  # 保存路径，存放检查点和其他输出文件\n",
    "        evaluation_strategy='steps',  # 每50steps结束后进行评价\n",
    "        eval_steps=50,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        report_to=\"tensorboard\",\n",
    "        # warmup_steps=500,  # 热身步数\n",
    "        # weight_decay=0.01,  # 权重衰减\n",
    "        learning_rate=2e-5,  # 初始学习率\n",
    "        per_device_train_batch_size=64,  # 训练批次大小\n",
    "        per_device_eval_batch_size=64,  # 测试批次大小\n",
    "        num_train_epochs=4,  # 训练轮数\n",
    "\n",
    "    )\n",
    "\n",
    "    # 定义训练器\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=tokenized_datasets['train'],\n",
    "        eval_dataset=tokenized_datasets[\"dev\"],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # 开始训练\n",
    "    trainer.train()\n",
    "    \n",
    "\n",
    "    # 训练完成以后的测试集评价\n",
    "    trainer.evaluate(eval_dataset=tokenized_datasets['test'])\n",
    "\n",
    "\n",
    "train_model(model_path, class_num=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.下游分类任务\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00289984, 0.01847867, 0.9786214 ],\n",
       "       [0.00311616, 0.00427782, 0.99260604],\n",
       "       [0.00322466, 0.03438428, 0.962391  ],\n",
       "       [0.00397229, 0.02145342, 0.97457427],\n",
       "       [0.03447386, 0.9541805 , 0.01134567]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# 根据保存的模型进行预测\n",
    "\n",
    "Cloud_Saved_Path = '/content/gdrive/MyDrive/Saved_Models/FinBERT_L-12_H-768_A-12/checkpoint-350/'\n",
    "\n",
    "Local_Saved_Path = '/content/LocalSaved/'\n",
    "\n",
    "Local_Saved_Name = 'FinBERT_L-12_H-768_A-12/'\n",
    "\n",
    "# 加载云盘中训练好的模型到本地\n",
    "\n",
    "\n",
    "def save_cloud_model():\n",
    "    if not os.path.exists(Local_Saved_Path):\n",
    "        os.makedirs(Local_Saved_Path)\n",
    "\n",
    "    import shutil\n",
    "    # 下载模型\n",
    "\n",
    "    try:\n",
    "        shutil.copytree(Cloud_Saved_Path, Local_Saved_Path+Local_Saved_Name)\n",
    "    except FileExistsError as e:\n",
    "        pass\n",
    "\n",
    "\n",
    "save_cloud_model()\n",
    "\n",
    "# 执行预测\n",
    "\n",
    "\n",
    "def predict_from_list(texts_list: list) -> np.ndarray:\n",
    "\n",
    "    # 模型路径\n",
    "    model_path = Local_Saved_Path+Local_Saved_Name\n",
    "\n",
    "    # 获取预训练的模型\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        model_path, num_labels=3)  # 3分类\n",
    "\n",
    "    # 用分词器进行预处理\n",
    "    encoded = tokenizer(texts_list, truncation=True,\n",
    "                        padding='max_length', max_length=32, return_tensors='pt')\n",
    "\n",
    "    # 传入要预测的任务到分类模型\n",
    "    out = model(**encoded)\n",
    "\n",
    "    # softmax输出每一类的概率\n",
    "    probs = out.logits.softmax(dim=-1)\n",
    "\n",
    "    return probs.detach().numpy()\n",
    "\n",
    "\n",
    "texts = [\n",
    "    '枪击案嫌犯称最初目标并非安倍',\n",
    "    '重庆一特斯拉失控 致多人伤亡',\n",
    "    '湖南一医院坐椅子收费10元',\n",
    "    '海航客机突发故障断电 机舱如蒸桑拿',\n",
    "    '外卖小哥考上上海交大研究生',\n",
    "]\n",
    "\n",
    "predict_from_list(texts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
